{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92e9ca0-6968-418a-beca-204c2e3ea7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers, callbacks\n",
    "from sklearn.datasets import fetch_openml \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03854dd8-f564-47d9-b733-b4f2bb8730b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = fetch_openml('Kuzushiji-MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a25cf6b-28d0-4d72-ae2a-f585ed6aaeb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, t = data.data.to_numpy(), data.target.to_numpy()\n",
    "X = X/255. # accomplishes the scaling\n",
    "X = X.reshape((70000,28,28))\n",
    "t = t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bdfd1-7661-418f-8105-e5efc5c83700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[34566], cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57488b11-a50a-4b6f-91cb-c60c1618c3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, t_train, t_test = X[:60000], X[60000:], t[:60000], t[60000:]\n",
    "X_train, X_val, t_train, t_val = train_test_split(X_train, t_train, \n",
    "                                                  test_size = 0.20,\n",
    "                                                  stratify = t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab5c8a5a-c669-4af2-a12a-32ddae55ab29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters to tune: learning rate, beta values, bottleneck size, \n",
    "lr = 0.005\n",
    "B1 = 0.9\n",
    "B2 = 0.999\n",
    "bottleneck = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c70e8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom SAE loss\n",
    "# class SAEloss(tf.keras.losses.Loss):\n",
    "#     def __init__(self, name=\"SAE_loss\"):\n",
    "#         # You can initialize any additional parameters for your loss here\n",
    "#         super().__init__(name=name)\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         This method defines the loss computation. The output of this function will\n",
    "#         be used to calculate the loss during training.\n",
    "\n",
    "#         Args:\n",
    "#             y_true: Tensor of true labels.\n",
    "#             y_pred: Tensor of predicted values.\n",
    "\n",
    "#         Returns:\n",
    "#             A scalar Tensor representing the loss value.\n",
    "#         \"\"\"\n",
    "#         # Example of Mean Squared Error (MSE) loss\n",
    "#         loss_value = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "#         # Optionally, you can add more loss methods based on specific conditions\n",
    "#         # Example: Adding a regularization term\n",
    "#         # regularization_loss = some_regularization_term(y_pred)\n",
    "\n",
    "#         # If you have multiple components in your loss, you can return their sum\n",
    "#         # return loss_value + regularization_loss\n",
    "        \n",
    "#         return loss_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6eecbbd0-0c7f-4197-902d-3892217436b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = models.Sequential([\n",
    "    layers.Flatten(input_shape=[28,28]),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(bottleneck,  activation='relu', use_bias=False)\n",
    "])\n",
    "\n",
    "decoder = models.Sequential([\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(28*28, use_bias=False),\n",
    "    layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "autoencoder = models.Sequential([encoder, decoder])\n",
    "\n",
    "autoencoder.compile(loss=losses.MeanSquaredError(reduction=\"sum_over_batch_size\"), \n",
    "                    optimizer=optimizers.Adam(learning_rate=lr,\n",
    "                    beta_1=B1,\n",
    "                    beta_2=B2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa596248-6f0d-44ae-a377-868966529ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0675 - val_loss: 0.0570\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0549 - val_loss: 0.0543\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0525 - val_loss: 0.0517\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0513 - val_loss: 0.0505\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0508 - val_loss: 0.0508\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0503 - val_loss: 0.0510\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0501 - val_loss: 0.0512\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0497 - val_loss: 0.0510\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0495 - val_loss: 0.0513\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0495 - val_loss: 0.0498\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0493 - val_loss: 0.0497\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0491 - val_loss: 0.0493\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0493 - val_loss: 0.0504\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0490 - val_loss: 0.0497\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0491\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0492\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0485\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0491\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0490 - val_loss: 0.0492\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0503\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0500\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0499\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0490\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0495\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0486 - val_loss: 0.0498\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0488\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0487 - val_loss: 0.0494\n"
     ]
    }
   ],
   "source": [
    "callback1=callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train, epochs=30, \n",
    "                    validation_data=(X_val, X_val),\n",
    "                    callbacks=callback1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5a6dd",
   "metadata": {},
   "source": [
    "# PCA as information for the bottleneck tuning\n",
    "While SAE offers increased functionality to PCA by creating weights that can be used in other functions such as VAE generation, at its core the SAE training serves the same function as PCA: dimensionality reduction. To this end, to inform our bottleneck tuning we can use PCA on the data and figure out how many dimensions we can remove while keeping 95% of the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5899c0d2-f5d2-418c-ae7d-d25c0a04f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA to find out how much dimensionality reduction we can do\n",
    "from sklearn.decomposition import PCA\n",
    "expVar1 = 0.95\n",
    "expVar2 = 0.99\n",
    "expVar3 = 0.999\n",
    "\n",
    "\n",
    "avg_dims1 = 0\n",
    "avg_dims2 = 0\n",
    "avg_dims3 = 0\n",
    "\n",
    "\n",
    "\n",
    "for iii in range(X_train.shape[0]):\n",
    "    \n",
    "    my_pca1 = PCA(n_components = expVar1)\n",
    "    my_pca2 = PCA(n_components = expVar2)\n",
    "    my_pca3 = PCA(n_components = expVar3)\n",
    "\n",
    "    my_pca1.fit(X_train[iii,:,:])\n",
    "    my_pca2.fit(X_train[iii,:,:])\n",
    "    my_pca3.fit(X_train[iii,:,:])\n",
    "    \n",
    "    avg_dims1 = avg_dims1 + len(my_pca1.explained_variance_ratio_)\n",
    "    avg_dims2 = avg_dims2 + len(my_pca2.explained_variance_ratio_)\n",
    "    avg_dims3 = avg_dims3 + len(my_pca3.explained_variance_ratio_)\n",
    "    \n",
    "avg_dims1 = avg_dims1 / X_train.shape[0]\n",
    "avg_dims2 = avg_dims2 / X_train.shape[0]\n",
    "avg_dims3 = avg_dims3 / X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25ed12ca-fefb-4a4b-a72c-bfe85f0c546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1459375\n",
      "11.061916666666667\n",
      "15.506791666666667\n"
     ]
    }
   ],
   "source": [
    "print(avg_dims1)\n",
    "print(avg_dims2)\n",
    "print(avg_dims3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fae1addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 3 autoencoders with these bottleneck layers and see the results\n",
    "lr = 0.001\n",
    "B1 = 0.9\n",
    "B2 = 0.999\n",
    "bottleneck1 = 8\n",
    "bottleneck2 = 12\n",
    "bottleneck3 = 16\n",
    "\n",
    "encoder1 = models.Sequential([\n",
    "    layers.Flatten(input_shape=[28,28]),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(bottleneck1,  activation='relu', use_bias=False)\n",
    "])\n",
    "\n",
    "encoder2 = models.Sequential([\n",
    "    layers.Flatten(input_shape=[28,28]),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(bottleneck2,  activation='relu', use_bias=False)\n",
    "])\n",
    "\n",
    "encoder3 = models.Sequential([\n",
    "    layers.Flatten(input_shape=[28,28]),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(bottleneck3,  activation='relu', use_bias=False)\n",
    "])\n",
    "\n",
    "decoder1 = models.Sequential([\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(28*28, use_bias=False),\n",
    "    layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "decoder2 = models.Sequential([\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(28*28, use_bias=False),\n",
    "    layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "decoder3 = models.Sequential([\n",
    "    layers.Dense(200,  activation='relu', use_bias=False),\n",
    "    layers.Dense(800,  activation='relu', use_bias=False),\n",
    "    layers.Dense(28*28, use_bias=False),\n",
    "    layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "autoencoder1 = models.Sequential([encoder1, decoder1])\n",
    "autoencoder2 = models.Sequential([encoder2, decoder2])\n",
    "autoencoder3 = models.Sequential([encoder3, decoder3])\n",
    "\n",
    "autoencoder1.compile(loss=losses.MeanSquaredError(reduction=\"sum_over_batch_size\"), \n",
    "                    optimizer=optimizers.Adam(learning_rate=lr,\n",
    "                    beta_1=B1,\n",
    "                    beta_2=B2))\n",
    "\n",
    "autoencoder2.compile(loss=losses.MeanSquaredError(reduction=\"sum_over_batch_size\"), \n",
    "                    optimizer=optimizers.Adam(learning_rate=lr,\n",
    "                    beta_1=B1,\n",
    "                    beta_2=B2))\n",
    "\n",
    "autoencoder3.compile(loss=losses.MeanSquaredError(reduction=\"sum_over_batch_size\"), \n",
    "                    optimizer=optimizers.Adam(learning_rate=lr,\n",
    "                    beta_1=B1,\n",
    "                    beta_2=B2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b824d7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0774 - val_loss: 0.0705\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0670 - val_loss: 0.0649\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0634 - val_loss: 0.0627\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0614 - val_loss: 0.0614\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0592 - val_loss: 0.0589\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0573 - val_loss: 0.0574\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0560 - val_loss: 0.0568\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0551 - val_loss: 0.0560\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0543 - val_loss: 0.0557\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0538 - val_loss: 0.0551\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0532 - val_loss: 0.0552\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0529 - val_loss: 0.0547\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0525 - val_loss: 0.0546\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0522 - val_loss: 0.0543\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0519 - val_loss: 0.0542\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0516 - val_loss: 0.0540\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0514 - val_loss: 0.0538\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0512 - val_loss: 0.0540\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0510 - val_loss: 0.0537\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0509 - val_loss: 0.0538\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0507 - val_loss: 0.0536\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0505 - val_loss: 0.0534\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0504 - val_loss: 0.0533\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0502 - val_loss: 0.0534\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0502 - val_loss: 0.0538\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0500 - val_loss: 0.0534\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0499 - val_loss: 0.0532\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0498 - val_loss: 0.0532\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0498 - val_loss: 0.0534\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0496 - val_loss: 0.0534\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0495 - val_loss: 0.0532\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0495 - val_loss: 0.0532\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0494 - val_loss: 0.0531\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0493 - val_loss: 0.0532\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0492 - val_loss: 0.0532\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0491 - val_loss: 0.0532\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0491 - val_loss: 0.0533\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0490 - val_loss: 0.0531\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0533\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0529\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0489 - val_loss: 0.0531\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0488 - val_loss: 0.0530\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0487 - val_loss: 0.0531\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0487 - val_loss: 0.0533\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0487 - val_loss: 0.0532\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0486 - val_loss: 0.0530\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0486 - val_loss: 0.0532\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0486 - val_loss: 0.0529\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0485 - val_loss: 0.0532\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0484 - val_loss: 0.0534\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0633 - val_loss: 0.0562\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0536 - val_loss: 0.0527\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0508 - val_loss: 0.0508\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0491 - val_loss: 0.0495\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0480 - val_loss: 0.0488\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0471 - val_loss: 0.0484\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0464 - val_loss: 0.0480\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0458 - val_loss: 0.0475\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0454 - val_loss: 0.0472\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0450 - val_loss: 0.0469\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0446 - val_loss: 0.0468\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0443 - val_loss: 0.0465\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0441 - val_loss: 0.0467\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0438 - val_loss: 0.0465\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0436 - val_loss: 0.0463\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0434 - val_loss: 0.0463\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0432 - val_loss: 0.0462\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0430 - val_loss: 0.0462\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0429 - val_loss: 0.0463\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0428 - val_loss: 0.0461\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0426 - val_loss: 0.0461\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0425 - val_loss: 0.0459\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0424 - val_loss: 0.0462\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0423 - val_loss: 0.0462\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0422 - val_loss: 0.0459\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0421 - val_loss: 0.0460\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0420 - val_loss: 0.0460\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0419 - val_loss: 0.0458\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0418 - val_loss: 0.0459\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0417 - val_loss: 0.0459\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0417 - val_loss: 0.0458\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0416 - val_loss: 0.0457\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0415 - val_loss: 0.0458\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0414 - val_loss: 0.0457\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0414 - val_loss: 0.0459\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0413 - val_loss: 0.0459\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0412 - val_loss: 0.0459\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0412 - val_loss: 0.0460\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0411 - val_loss: 0.0460\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0411 - val_loss: 0.0459\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0411 - val_loss: 0.0461\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0410 - val_loss: 0.0458\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0631 - val_loss: 0.0541\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0518 - val_loss: 0.0506\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0491 - val_loss: 0.0490\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0475 - val_loss: 0.0482\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0464 - val_loss: 0.0470\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0456 - val_loss: 0.0469\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0449 - val_loss: 0.0465\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0444 - val_loss: 0.0461\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0439 - val_loss: 0.0456\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0435 - val_loss: 0.0457\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0432 - val_loss: 0.0453\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0429 - val_loss: 0.0452\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0427 - val_loss: 0.0451\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0424 - val_loss: 0.0451\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0422 - val_loss: 0.0448\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0420 - val_loss: 0.0447\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0419 - val_loss: 0.0449\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0417 - val_loss: 0.0447\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0416 - val_loss: 0.0445\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0414 - val_loss: 0.0445\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0413 - val_loss: 0.0444\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0412 - val_loss: 0.0444\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0411 - val_loss: 0.0443\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0409 - val_loss: 0.0445\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0408 - val_loss: 0.0445\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0407 - val_loss: 0.0444\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0406 - val_loss: 0.0443\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0406 - val_loss: 0.0442\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0405 - val_loss: 0.0443\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0404 - val_loss: 0.0442\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0403 - val_loss: 0.0442\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0403 - val_loss: 0.0442\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0402 - val_loss: 0.0442\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0401 - val_loss: 0.0442\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0401 - val_loss: 0.0446\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0400 - val_loss: 0.0444\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0399 - val_loss: 0.0443\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0399 - val_loss: 0.0444\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0398 - val_loss: 0.0441\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0398 - val_loss: 0.0441\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0397 - val_loss: 0.0443\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0397 - val_loss: 0.0441\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0396 - val_loss: 0.0442\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0396 - val_loss: 0.0440\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0395 - val_loss: 0.0444\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0395 - val_loss: 0.0442\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0394 - val_loss: 0.0442\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0394 - val_loss: 0.0445\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0394 - val_loss: 0.0442\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0393 - val_loss: 0.0443\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0392 - val_loss: 0.0444\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0392 - val_loss: 0.0441\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0392 - val_loss: 0.0445\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0392 - val_loss: 0.0441\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "callback1=callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history1 = autoencoder1.fit(X_train, X_train, epochs=100, \n",
    "                    validation_data=(X_val, X_val),\n",
    "                    callbacks=callback1)\n",
    "\n",
    "history2 = autoencoder2.fit(X_train, X_train, epochs=100, \n",
    "                    validation_data=(X_val, X_val),\n",
    "                    callbacks=callback1)\n",
    "\n",
    "history3 = autoencoder3.fit(X_train, X_train, epochs=100, \n",
    "                    validation_data=(X_val, X_val),\n",
    "                    callbacks=callback1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe403f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to add noise to the input\n",
    "def add_gaussian_noise(input_image, mean=0.0, stddev=0.1):\n",
    "    # Add Gaussian noise with mean `mean` and standard deviation `stddev`\n",
    "    noise = tf.random.normal(shape=tf.shape(input_image), mean=mean, stddev=stddev)\n",
    "    noisy_image = input_image + noise\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ed03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
